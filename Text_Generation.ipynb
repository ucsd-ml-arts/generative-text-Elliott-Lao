{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mainly copied and adapted from the Text Generation RNN example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dataset\n",
    "path_to_file = \"corpus.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1334722 characters\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      " \n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//seq_length\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "  print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'                    DAVID ATTENBOROUGH: The natural world is full  of extraordinarily shaped creature'\n",
      "'s,  but how have the stretched bodies of some given them an edge?  I have had the fortune to meet  so'\n",
      "\"me of the planet's most enchanting creatures, but some  stand out more than others, because of their \"\n",
      "'intriguing biology.\\n  Our knowledge of some of these creatures extends back centuries.\\n  Others we ha'\n",
      "'ve discovered more recently.\\n  In this series, I share their stories  and reveal why they are conside'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "  print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size \n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = examples_per_epoch//BATCH_SIZE\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences, \n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead, \n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension \n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.test.is_gpu_available(): #GPU\n",
    "  rnn = tf.keras.layers.CuDNNGRU\n",
    "else:\n",
    "  import functools\n",
    "  rnn = functools.partial(\n",
    "    tf.keras.layers.GRU, recurrent_activation='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, \n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    rnn(rnn_units,\n",
    "        return_sequences=True, \n",
    "        recurrent_initializer='glorot_uniform',\n",
    "        stateful=True),\n",
    "    # tack on two RNN layers \n",
    "    rnn(rnn_units,\n",
    "        return_sequences=True, \n",
    "        recurrent_initializer='glorot_uniform',\n",
    "        stateful=True),\n",
    "    \n",
    "    rnn(rnn_units,\n",
    "        return_sequences=True, \n",
    "        recurrent_initializer='glorot_uniform',\n",
    "        stateful=True),\n",
    "      \n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab), \n",
    "  embedding_dim=embedding_dim, \n",
    "  rnn_units=rnn_units, \n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 89) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1): \n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           22784     \n",
      "_________________________________________________________________\n",
      "cu_dnngru (CuDNNGRU)         (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "cu_dnngru_1 (CuDNNGRU)       (64, None, 1024)          6297600   \n",
      "_________________________________________________________________\n",
      "cu_dnngru_2 (CuDNNGRU)       (64, None, 1024)          6297600   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 89)            91225     \n",
      "=================================================================\n",
      "Total params: 16,647,513\n",
      "Trainable params: 16,647,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 89)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.4890766\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "#   return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "  return tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\") \n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = tf.train.AdamOptimizer(),\n",
    "    loss = loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './text100'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/45\n",
      "208/208 [==============================] - 31s 148ms/step - loss: 2.3123\n",
      "Epoch 2/45\n",
      "208/208 [==============================] - 30s 146ms/step - loss: 1.4729\n",
      "Epoch 3/45\n",
      "208/208 [==============================] - 29s 141ms/step - loss: 1.2796\n",
      "Epoch 4/45\n",
      "208/208 [==============================] - 30s 144ms/step - loss: 1.1967\n",
      "Epoch 5/45\n",
      "208/208 [==============================] - 30s 143ms/step - loss: 1.1388\n",
      "Epoch 6/45\n",
      "208/208 [==============================] - 29s 142ms/step - loss: 1.0914\n",
      "Epoch 7/45\n",
      "208/208 [==============================] - 30s 145ms/step - loss: 1.0492\n",
      "Epoch 8/45\n",
      "208/208 [==============================] - 29s 140ms/step - loss: 1.0104\n",
      "Epoch 9/45\n",
      "208/208 [==============================] - 30s 144ms/step - loss: 0.9709\n",
      "Epoch 10/45\n",
      "208/208 [==============================] - 29s 138ms/step - loss: 0.9343\n",
      "Epoch 11/45\n",
      "208/208 [==============================] - 29s 141ms/step - loss: 0.8975\n",
      "Epoch 12/45\n",
      "208/208 [==============================] - 30s 146ms/step - loss: 0.8645\n",
      "Epoch 13/45\n",
      "208/208 [==============================] - 29s 142ms/step - loss: 0.8316\n",
      "Epoch 14/45\n",
      "208/208 [==============================] - 29s 140ms/step - loss: 0.7994\n",
      "Epoch 15/45\n",
      "208/208 [==============================] - 29s 137ms/step - loss: 0.7728\n",
      "Epoch 16/45\n",
      "208/208 [==============================] - 30s 143ms/step - loss: 0.7465\n",
      "Epoch 17/45\n",
      "208/208 [==============================] - 31s 150ms/step - loss: 0.7225\n",
      "Epoch 18/45\n",
      "208/208 [==============================] - 30s 145ms/step - loss: 0.7032\n",
      "Epoch 19/45\n",
      "208/208 [==============================] - 31s 150ms/step - loss: 0.6837\n",
      "Epoch 20/45\n",
      "208/208 [==============================] - 31s 149ms/step - loss: 0.6654\n",
      "Epoch 21/45\n",
      "208/208 [==============================] - 30s 146ms/step - loss: 0.6525\n",
      "Epoch 22/45\n",
      "208/208 [==============================] - 30s 146ms/step - loss: 0.6366\n",
      "Epoch 23/45\n",
      "208/208 [==============================] - 31s 151ms/step - loss: 0.6257\n",
      "Epoch 24/45\n",
      "208/208 [==============================] - 30s 145ms/step - loss: 0.6137\n",
      "Epoch 25/45\n",
      "208/208 [==============================] - 31s 149ms/step - loss: 0.6063\n",
      "Epoch 26/45\n",
      "208/208 [==============================] - 31s 151ms/step - loss: 0.5959\n",
      "Epoch 27/45\n",
      "208/208 [==============================] - 30s 146ms/step - loss: 0.5913\n",
      "Epoch 28/45\n",
      "208/208 [==============================] - 30s 146ms/step - loss: 0.5840\n",
      "Epoch 29/45\n",
      "208/208 [==============================] - 29s 141ms/step - loss: 0.5806\n",
      "Epoch 30/45\n",
      "208/208 [==============================] - 30s 144ms/step - loss: 0.5762\n",
      "Epoch 31/45\n",
      "208/208 [==============================] - 30s 145ms/step - loss: 0.5729\n",
      "Epoch 32/45\n",
      "208/208 [==============================] - 30s 145ms/step - loss: 0.5680\n",
      "Epoch 33/45\n",
      "208/208 [==============================] - 30s 142ms/step - loss: 0.5665\n",
      "Epoch 34/45\n",
      "208/208 [==============================] - 30s 146ms/step - loss: 0.5646\n",
      "Epoch 35/45\n",
      "208/208 [==============================] - 29s 141ms/step - loss: 0.5658\n",
      "Epoch 36/45\n",
      "208/208 [==============================] - 31s 149ms/step - loss: 0.5655\n",
      "Epoch 37/45\n",
      "208/208 [==============================] - 32s 152ms/step - loss: 0.5629\n",
      "Epoch 38/45\n",
      "208/208 [==============================] - 30s 146ms/step - loss: 0.5649\n",
      "Epoch 39/45\n",
      "208/208 [==============================] - 30s 145ms/step - loss: 0.5650\n",
      "Epoch 40/45\n",
      "208/208 [==============================] - 31s 149ms/step - loss: 0.5670\n",
      "Epoch 41/45\n",
      "208/208 [==============================] - 30s 144ms/step - loss: 0.5687\n",
      "Epoch 42/45\n",
      "208/208 [==============================] - 31s 148ms/step - loss: 0.5705\n",
      "Epoch 43/45\n",
      "208/208 [==============================] - 31s 149ms/step - loss: 0.5726\n",
      "Epoch 44/45\n",
      "208/208 [==============================] - 30s 146ms/step - loss: 0.5778\n",
      "Epoch 45/45\n",
      "208/208 [==============================] - 30s 144ms/step - loss: 0.5831\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset.repeat(), epochs=EPOCHS, steps_per_epoch=steps_per_epoch, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./text100/ckpt_45'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1) \n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            22784     \n",
      "_________________________________________________________________\n",
      "cu_dnngru_3 (CuDNNGRU)       (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "cu_dnngru_4 (CuDNNGRU)       (1, None, 1024)           6297600   \n",
      "_________________________________________________________________\n",
      "cu_dnngru_5 (CuDNNGRU)       (1, None, 1024)           6297600   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 89)             91225     \n",
      "=================================================================\n",
      "Total params: 16,647,513\n",
      "Trainable params: 16,647,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 4500\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing) \n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  temperature = 0.7 #reduce temperature to 0.7 non words appear at higher temperatures \n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a multinomial distribution to predict the word returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
    "      \n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "      \n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTENBOROUGH: The power of the sun  drives the seasons, transformed our image and understandably out of here.\n",
      "  They are very rare only a few days,  but it doesn't make it through the sponge's walls.\n",
      "  His run itself up here and then release them.\n",
      "  He was hardly be new life.\n",
      "  He's a bit of a two-year-old son, Tatu can't grab them.\n",
      "  So, why both parents have to tackle the difficulties of life  deeper into the water.\n",
      "  It's a calf to succeed here,  but another week planted pouch of our pain.\n",
      "  It's very rare for another two years.\n",
      "  The Austrian one lasted just a few days.\n",
      "  Until now, at last, tell the rest combined.\n",
      "  Unlike made from the brine, super-pods breen,  the length of a butterfly's life cycle,  or the strange biology of the emperor penguin.\n",
      "  Some of these creatures were surrounded by myths and misunderstanding of forest  and the cubs' cluckications in particular type of sperm  and they've now disappeared with death.\n",
      "  But carotene is not only seaweed on which to live.\n",
      "  And landed on the reach of other birds,  and the end the great areas of food supplies.\n",
      "  And when they do find that,  we'd the distance travelled from Europe to Shackleton's camera  to European society, we are getting ready to mate.\n",
      "  If they feed amongst the front of the sub of it, hard every day between the couple  by taling birds to rid them, yell the monkeys know exactly,  the power of the sun drives the seasons,  transforming our planet.\n",
      "  Vast movements of ocean and air currents  bring dramatic change throughout the year.\n",
      "  And in a few special places, these corals were now transformed.\n",
      "  The small naked mole rats remain myths and man mean the calf, runs in the summer melt,  and can now really a metre above his harema.\n",
      "  So the birds here,  indict behind the rest of the reefs  that had been six months since the cuckoo would do very much the same basic destroy into the bones.\n",
      "  The bears know that it's probably made of calculating the mandrake's shaped creatures  that have made an even drink the blood to them.\n",
      "  As the tide falls still full of frozen water.\n",
      "  The colony is in the South Pacific.\n",
      "  Prey may be that they could be our female that we've ever seen in my life.\n",
      "  He's a bit of a mysterious world.\n",
      "  We've seen her what's happened to film the hunters.\n",
      "  They are setting out to the dolphin as close.\n",
      "  The team were everythy males and sea  can be traced back to three very special giraffes  and the storm returns from their mouths.\n",
      "  And as the warm, cultures living at 80 miles an hour.\n",
      "  Will you cover thousands of miles across space,  solar winds, attracted by a meter  the window of opportunity to win off,  and then it will be only three months since the hunters  must do together to spawn.\n",
      "  But in this part of the world,  primates can itslly,  and is so susceptic salmon.\n",
      "  But its time, a vast search prey are always break up.\n",
      "  The baby is nearly over 50 times,  but it's also a reptilian monster mistake.\n",
      "  The local people that could have to cover thousands of miles away.\n",
      "  (MARMOTO  That was amazing.\n",
      "  Handed on the east  was the study of its own weight in here  to the work  of deep into his nerve.\n",
      "  What is more conspicuous  if he was a ceiling areasoned sea monster.\n",
      "  The water they take up the mountain range of sledgehall has to continue to grow.\n",
      "  300s elsewhere.\n",
      "  It's the first time they return here  for those that can will they be able to see the process of finding corals,  have learned how to conjure water out.\n",
      "  As the Antarctic south, in the desert,  the greatest seasonal change is happening northwards.\n",
      "  But there are five and a half to three weeks old.\n",
      "  The small ants don't drive them no leftovers today.\n",
      "  The prize for the owners to stay with their mothers,  some were extremely choosy.\n",
      "  It may be many detailed dramatically it behaviour,  the diving birds are that accelerates this body from South Georgia's beaches in the desert.\n",
      "  Here we can only strike the worst of the world's greatest line,  the power attracts a barricact.\n",
      "  And even the conflict,  they must accept the trap.\n",
      "  When the tide is low the team  to climb the turtles only started from the den.\n",
      "  They have strong, dextrous hands and feet,  and cranning across the dunes.\n",
      "  Finally, home to the grub.\n",
      "  Even the ice reflexising the team in the deep ocean, weeks.\n",
      "  The orang's diet is seal.\n",
      "  These are the biggest army of Australia,  hundreds of miles to rid them,  is a stoplight look at something to eat here,  but access to high food, better than sexual marvellous \n"
     ]
    }
   ],
   "source": [
    "# print(generate_text(model, start_string=u\"ROMEO\"))\n",
    "print(generate_text(model, start_string=u\"ATTENBOROUGH:\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
